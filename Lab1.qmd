---
title: "Machine Learning: Mathematical Theory and Applications"
subtitle: "Computer lab 1"
author: 
  - Matias Quiroz
date: last-modified
format: 
  html:
    self-contained: true
toc: true
execute:
  error: false
language: 
  title-block-author-single: " "
theme: Default
title-block-banner-color: Primary
editor: visual
---

```{=html}
<style>
.boxed-text {
  border: 2px solid black;
  padding: 10px;
  margin: 10px 0;
}
</style>
```

::: callout-warning
The instructions in this lab assume that the following packages are installed:

-   `splines`

-   `caret`

Packages may be installed by executing `install.packages('packagename')`, where `'packagename'` is the name of the package, e.g. `'splines'`. You may have to install additional packages to solve the computer lab. If you want this file to run locally on your computer, you have to change some paths to where the data are stored (see below).
:::

## Introduction

This computer lab corresponds to Module 1 and treats topics such as polynomial and spline regression, regularisation, cross-validation, regression and decision trees, and classification methods.

------------------------------------------------------------------------

::: callout-note
### Instructions

In this computer lab, you will work individually but you are free to discuss with other students, especially the one student you will work with on Computer lab 2! However, you have to hand in your own set of solutions. **It is strictly forbidden to copy anyone else's code or solutions**. This is regarded as academic misconduct and may have serious consequences for you. **Irresponsible use of AI tools (such as ChatGPT) is not recommended**. Keep in mind that such tools are not permitted in the invigilated final project.

This computer lab is worth a total of 15 marks (the subject has a maximum of 100 marks). The maximum marks for a given problem is shown within parenthesis at the top of the section. The marking rubric is available on Canvas. The problems you should solve are marked with the symbol ðŸ’ª and surrounded by a box. Hand in the solutions and programming codes in any form you like, however, I will not be accepting any obscure .docx or .xlsx files (convert to pdf if needed). You are encouraged to learn how to use Quarto to generate scientific and technical reports. (This computer lab is typeset in Quarto.) A Quarto template on Canvas is provided for this purpose. If you use Quarto, hand in the report in the form of a html document generated by Quarto. **Before submitting, carefully check that the document compiles without any errors**. Use properly formatted figures and programming code.
:::

::: callout-warning
Submitting poorly written reports may result in loss of marks. The same applies to poorly formatted reports (including poorly formatted code/figures, unnecessarily long outputs, etc.).
:::

## 1. Polynomial regression for bike rental data (2 marks)

In this problem, we consider modelling the number of rides each hour for a bike rental company. The raw data are stored in the file `bike_rental_hourly.csv`, which can be downloaded from the Canvas page of the course[^1].

[^1]: The original data come from this [source](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset).

The dataset contains 17379 hourly observations over the two-year period January 1, 2011 - December 31, 2012. The dataset contains many features that may be useful for predicting the number of rides, such as the hour of the day, temperature, humidity, season, weekday, etc. In this section, we consider modelling the logarithm of the number of rides as a function of the time of the day (scaled to the interval \[0,1\], see below). The following code reads in the data (note that you have to change to the path were you stored the file!) and creates the variables of interest (`log_cnt` and `hour`)

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
bike_data <- read.csv('C:/Users/TSC/Desktop/UTS/Sem 1/Machine Learning/Cmp Lab 1/bike_rental_hourly.csv')
str(bike_data)
head(bike_data)
bike_data$log_cnt <- log(bike_data$cnt)
bike_data$hour <- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM
plot(log_cnt ~ hour, data = bike_data, col = "cornflowerblue")
```

The number of rides seem to peak in the morning (8 AM $8/23 \approx 0.35$ ) and after work (6 PM $18/23 \approx 0.78$).

We start by fitting a polynomial of order 2 to a training dataset consisting of Feb 1, 2011 - March 31, 2011. We use the following two months (April 1, 2011 - May 31, 2011) as test dataset to evaluate the predictions. The following code creates the training and test datasets.

```{r}
bike_data_train <- bike_data[bike_data$dteday >= as.Date("2011-02-01") & bike_data$dteday <=  as.Date("2011-03-31"), ]
bike_data_test <- bike_data[bike_data$dteday >= as.Date("2011-04-01") & bike_data$dteday <=  as.Date("2011-05-31"), ]
```

The following code estimates the model (a polynomial of order 2) using least squares and computes the root mean squared error (RMSE) for both the training data and the test data. Moreover, the code plots the training data, test data and the fitted polynomial in the same plot.

```{r}
y_train <- bike_data_train$log_cnt
y_test <- bike_data_test$log_cnt
p <- 2 # Order of polynomial
X_train <- cbind(1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE)) # Design mmatrix of features (including intercept)
beta_hat <- solve(t(X_train)%*%X_train)%*%t(X_train)%*%y_train

# Predict in-sample and compute RMSE
y_hat_train <- X_train%*%beta_hat 
RMSE_train <- sqrt(sum((y_train - y_hat_train)^2)/length(y_train))

# Predict out-of-sample and compute RMSE
X_test <- cbind(1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))
y_hat_test <- X_test%*%beta_hat
RMSE_test <- sqrt(sum((y_test - y_hat_test)^2)/length(y_test))

# Plot training data, test data, and fit on a fine grid.
plot(log_cnt ~ hour, data = bike_data_train, col = "cornflowerblue", ylim = c(0, 8))
lines(bike_data_test$hour, bike_data_test$log_cnt, type = "p", col = "lightcoral")
hours_grid <- seq(0, 1, length.out = 1000)
X_grid <- cbind(1, poly(hours_grid, p, raw = TRUE, simple = TRUE))
y_hat_grid <- X_grid%*%beta_hat
lines(hours_grid, y_hat_grid, lty = 1, col = "lightcoral")
legend(x = "topleft", pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c("cornflowerblue", "lightcoral",  "lightcoral"), legend=c("Train", "Test", "Fitted curve"))

```

::: boxed-text
#### ðŸ’ª Problem 1.1

Fit a polynomial of order 8 using least squares, without using R functions such as `lm()`. Plot the training data, test data and the fitted polynomial in the same plot.
:::

```{r}
#=== 1.Prepare data
#Training data and test data are determined above in the problem requirement. 

y_train <- bike_data_train$log_cnt
y_test <- bike_data_test$log_cnt
p <- 8 #Order of polynominal

#=== 2.Design matrix of features (including intercept)
xpoly_train <- cbind(1, poly(bike_data_train$hour, p ,raw = TRUE, simple = TRUE))
xpoly_test <- cbind(1, poly(bike_data_test$hour, p , raw = TRUE, simple = TRUE))
beta_hat <- solve(t(xpoly_train)%*%xpoly_train)%*%t(xpoly_train)%*%y_train

#=== 3.Predict in-sample and out-of-sample y hat
ypoly_hat_train <- xpoly_train%*%beta_hat
ypoly_hat_test <- xpoly_test%*%beta_hat

#=== 3.Compute RMSE
RMSE_train_poly <- sqrt (mean((y_train - ypoly_hat_train)^2))
RMSE_test_poly <- sqrt(mean((y_test - ypoly_hat_test)^2))
cat("RMSE (Train):", RMSE_train_poly, "\n")
cat ("RMSE (Test):", RMSE_test_poly, "\n")

#=== 4.Plot training data, test data, and fit on a fine grid
plot (log_cnt ~ hour, data = bike_data_train, col = "cornflowerblue", ylim = c(0,8), main = "Polynominal of order 8")
lines(bike_data_test$hour, bike_data_test$log_cnt, type = "p", col = "lightcoral")

hours_grid <- seq(0,1, length.out = 1000) #create a prediction grid matrix, run from 0 to 1, which is the all the value of variable x in the range [0,1] 
x_grid <- cbind(1, poly(hours_grid, p, raw = TRUE, simple = TRUE))
y_hat_grid <- x_grid%*%beta_hat
lines (hours_grid, y_hat_grid, lty = 1, col = "red")

legend (x = "topleft", pch = c(1 , 1, NA), lty = c(NA, NA, 1), col = c("cornflowerblue", "lightcoral", "red"), legend = c ("Train", "Test", "Fitted curve (p=8)"))
```

::: boxed-text
#### ðŸ’ª Problem 1.2

Fit polynomials of varying order 1-10 using least squares, without using R functions such as `lm()`. Compute the RMSEs for the training and test data and plot them on the same figure as a function of the polynomial order. Are you underfitting or overfitting the data?
:::

```{r}
#=== 1. Prepare data
y_train <- bike_data_train$log_cnt
y_test <- bike_data_test$log_cnt

#=== 2.Create empty vectors to store RMSE
orders <- 1:10 #Order of polynominal
RMSEs_train <- numeric(length(orders))
RMSEs_test <- numeric(length(orders))

#=== 3. Loop over polynomial orders from 1 to 10
for (p in orders) { 
  #--- Desgin matrix
  
  x_train <- cbind (1, poly(bike_data_train$hour, p, raw = TRUE, simple = TRUE))
  x_test <- cbind (1, poly(bike_data_test$hour, p, raw = TRUE, simple = TRUE))
  
  #--- Predict in-sample and out-of-sample y hat
  beta_hat<- solve (t(x_train)%*%x_train)%*%t(x_train)%*%y_train
  y_hat_train <- x_train%*%beta_hat
  y_hat_test <- x_test%*%beta_hat
  
  #--- Compute RMSE 
  RMSEs_train[p] <- sqrt(mean((y_train - y_hat_train)^2))
  RMSEs_test [p] <- sqrt(mean((y_test - y_hat_test)^2))
  }

RMSEs_table <- data.frame(Polynomial_Order = orders, RMSEs_train, RMSEs_test)
print (RMSEs_table)

#=== 4. Plot RMSE of training data and test data
plot (RMSEs_train ~ orders, data = RMSEs_table, type = "b", col = "darkblue", pch = 18, ylim = c(0,1.5), main = "RMSEs Train vs RMSEs Test", ylab = "RMSEs", xlab = "Polynomial order")
lines (orders, RMSEs_test, type = "b", col = "darkred", pch = 19)
legend (x = "bottomleft", pch = c(18,19), lty = c (1,1), col = c("darkblue", "darkred"), legend = c("Train","Test"))

#=== 5. Conclusion
#RMSE of both Test and Train data decreases as the polynomial order increases, suggesting possible good fit at the polynomial order of 3 or 4.  Starting from the order 5, RMSE started to increase, suggesting possible overfitting

```

:::: boxed-text
#### ðŸ’ª Problem 1.3

Polynomials are global functions and their fit may be sensitive to outliers. Local fitting methods can sometimes be more robust. One such method is the `loess` method, which is a nonparametric method that fits locally weighted regressions to subsets of points and subsequently combines them to obtain a global fit. Use the `loess` function in R with the standard settings to fit a locally weighted regression to the training data. Is this method better than that in Problem 1.1? Plot the training data, test data and both fitted curves in the same plot.

::: callout-tip
The `predict()` function can be used on the object returned by `loess()`.
:::
::::

```{r}
#=== 1.Prepare data
y_train <- bike_data_train$log_cnt
y_test <- bike_data_test$log_cnt

#=== 2.LOESS model
  
  #--- Fit a LOESS model to the training data
  loess_fit <- loess(log_cnt ~ hour,data = bike_data_train)
  
  #--- Predict in-sample and out-of-sample y hat
  yhat_train_loess <- predict (loess_fit, newdata = bike_data_train)
  yhat_test_loess <- predict (loess_fit, newdata = bike_data_test)
  
  #--- Compute RMSE
  RMSE_train_loess <- sqrt (mean((y_train - yhat_train_loess)^2))
  RMSE_test_loess <- sqrt (mean ((y_test - yhat_test_loess)^2))

#=== 3.Plot comparison
plot (log_cnt ~ hour, data=bike_data_train, col = "cornflowerblue", pch = 1, ylim = c (0,8), main = "Polynominal vs LOESS fit")
lines(bike_data_test$hour, bike_data_test$log_cnt, type = "p", col = "lightcoral")

hours_grid <- seq(0,1, length.out = 1000)
yhat_grid_loess <- predict (loess_fit, newdata = data.frame(hour = hours_grid))
lines (hours_grid, y_hat_grid, lty = 1, col = "darkblue")
lines (hours_grid, yhat_grid_loess,lty = 1, col ="darkred")

legend (x = "topleft", pch = c(1,1,NA,NA), lty = c(NA, NA, 1, 1) , col = c("cornflowerblue","lightcoral", "darkblue", "darkred"), legend = c("Train", "Test", "Fitted curve", "LOESS fitted curve"))

#=== 5. Comparison with Model polynomial of order 8 and conclusion 
RMSE_comparison <- data.frame (Model = c("Polynominal (deg 8)", "LOESS"), RMSE_Train = c(RMSE_train_poly, RMSE_train_loess), RMSE_Test = c(RMSE_test_poly, RMSE_test_loess))

print (RMSE_comparison)
#Conclusion: 
```

## 2. Regularised spline regression for bike rental data (3 marks)

Using the same training and test datasets as above, we will fit spline regressions with L1 and L2 regularisations (also known as penalties).

We create a natural cubic spline basis function for the variable hour with 25 equally spaced knots between 0.05 and 0.95. We first fit a regression using least squares without regularisation. The following code does this and plots the spline fit together with the training and test data. Note that we are not adding an intercept to the design matrix, since this is taken care of by the argument `intercept=TRUE`.

```{r}
suppressMessages(library(splines))
knots <- seq(0.05, 0.95, length.out = 25)
X_train <- ns(bike_data_train$hour, knots = knots, intercept = TRUE)
X_test <- ns(bike_data_test$hour, knots = knots, intercept = TRUE)
beta_hat <- solve(t(X_train)%*%X_train)%*%t(X_train)%*%y_train

# Plot training data, test data, and spline fit on a fine grid.
plot(log_cnt ~ hour, data = bike_data_train, col = "cornflowerblue", ylim = c(0, 8))
lines(bike_data_test$hour, bike_data_test$log_cnt, type = "p", col = "lightcoral")
hours_grid <- seq(0, 1, length.out = 1000)
X_grid <- ns(hours_grid, knots = knots, intercept = TRUE) # cbind(1, ns(hours_grid, knots = knots))
y_hat_spline_grid <- X_grid%*%beta_hat
lines(hours_grid, y_hat_spline_grid, lty = 1, col = "lightcoral")
legend(x = "topleft", pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c("cornflowerblue", "lightcoral",  "lightcoral"), legend=c("Train", "Test", "Spline"))
```

We have deliberately chosen too many knots and no regularisation so that the fit has a large variance, which is clearly evident from the figure above. Your task in the next problem is to fit a regularised spline regression.

:::: boxed-text
#### ðŸ’ª Problem 2.1

Fit a spline regression to the training data with an L2 regularisation using a suitable value of $\lambda$, without using R functions such as `glmnet()`. Plot the fit together with the training and test data.

::: callout-tip
The least squares estimator when using an L2 penalty is known as the Ridge regression estimator. Moreover, to determine a suitable value of $\lambda$, one approach is to fit several models (using different $\lambda$ values, e.g. `lambda_grid <- seq(0, 1, length.out=100)`) and choose the value of $\lambda$ that minimises the root mean squared error of the test data.
:::
::::

```{r}
suppressMessages(library(splines))
knots <- seq (0.05, 0.95, length.out = 25)
#=== Design new matrix of feature using natural cubic spline
x_train <- ns (bike_data_train$hour, knots = knots, intercept = TRUE)
x_test <- ns (bike_data_test$hour, knots = knots, intercept = TRUE)

#=== 1. Create lambda grid and empty vectors to store RMSE results
lambda_grid <- seq (0, 1, length.out = 100)
RMSE_spline_test <- numeric (length(lambda_grid))

#=== 2. Loop over 100 Lambda values to determine suitable Î»
for (i in 1:length(lambda_grid)) {
  lambda <- lambda_grid[i]
  I <- diag(ncol(x_train))
  
  #--- Ridge regression for each Î»
  beta_spline <- solve (t(x_train)%*%x_train + lambda*I)%*%t(x_train)%*%y_train
  yhat_test_spline <- x_test%*%beta_spline
  
  #--- Compute RMSE
  RMSE_spline_test [i] <- sqrt (mean((y_test - yhat_test_spline)^2)) }

RMSE_spline_table <- data.frame(Lambda = lambda_grid, RMSE_spline_test)
print (RMSE_spline_table)

#=== 3. Report the Lambda that minimizes the test RMSE
best_lambda <- lambda_grid[which.min(RMSE_spline_test)]
cat("Best Î» (min test RMSE)", best_lambda)

#=== 4. Refit the model using the best lambda
beta_best <- solve (t(x_train)%*%x_train + best_lambda*I)%*%t(x_train)%*%y_train

#=== 5. Plot on a fine grid
plot (log_cnt ~ hour, data = bike_data_train, col = "cornflowerblue", pch = 1, ylim = c(0,8), main = paste("Spline Regression with Î» =", round(best_lambda,4)))
lines (bike_data_test$hour, bike_data_test$log_cnt, type = "p", col ="lightcoral")

hours_grid <- seq(0,1, length.out = 1000)
xspline_grid <- ns (hours_grid, knots = knots, intercept = TRUE) #cbind (1, ns(hours_grid, knots = knots))
yspline_grid <- xspline_grid%*%beta_best
lines (hours_grid, yspline_grid, lty = 1, col ="darkred")
#Ridge Spline fitted curve

legend ( x = "topleft", pch = c(1,1, NA), lty = c (NA, NA, 1), col= c("cornflowerblue","lightcoral","darkred"),legend = c("Train", "Test", " Spline"))

```

:::: boxed-text
#### ðŸ’ª Problem 2.2

Use the package `glmnet` to fit a spline regression with an L2 regularisation using the same basis functions as the previous problem. Find the optimal $\lambda$ by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal $\lambda$) for the training and test data. Plot the fit together with the training and test data.

::: callout-tip
The `help()` function will be very useful here. Another useful function is `cv.glmnet()`, where cv stands for cross-validation. The argument `alpha` (to the `cv.glmnet()` function) is key. Finally, the `predict()` function can be used on the object returned by `cv.glmnet()` and the argument `s` (to the `predict()` function) controls which $\lambda$ to use.
:::
::::

```{r}
suppressMessages(library(glmnet))

#=== 1. Prepare data
knots <- seq (0.05, 0.95, length.out = 25)
x_train <- ns (bike_data_train$hour, knots = knots, intercept = TRUE)
x_test <- ns (bike_data_test$hour, knots = knots, intercept = TRUE)

#=== 2. Fit Ridge regression with Cross-validation
set.seed (0)
cv_fit <- cv.glmnet(x = x_train, y = y_train, alpha = 0, nfolds = 10)
print (cv_fit)

#=== 3. Choosing the optimal Lambda using 1-SE rule
opt_lambda <- cv_fit$lambda.1se
cat ("Optimal lambda (1 - SE rule):", opt_lambda, "\n")

#=== 4. Predict fitted values with lambda.1se
yhat_train_cv <- predict (cv_fit, newx = x_train, s = "lambda.1se")
yhat_test_cv <- predict (cv_fit, newx = x_test, s = "lambda.1se")

#=== 5. Compute RMSE
RMSE_train_cv <- sqrt (mean((y_train - yhat_train_cv)^2))
RMSE_test_cv <- sqrt (mean((y_test - yhat_test_cv)^2))
cat("RMSE (Train):", RMSE_train_cv, "\n")
cat("RMSE (Test):", RMSE_test_cv, "\n")

#=== 6. Plot the fit 
plot (log_cnt ~ hour, data=bike_data_train, col = "cornflowerblue", ylim = c(0,8), main = paste ("Ridge (glmnet) with Î» = ", round(opt_lambda,4)))
lines (bike_data_test$hour, bike_data_test$log_cnt, type = "p", col ="lightcoral")

hours_grid <- seq(0,1, length.out = 1000)
x_cv_grid <- ns (hours_grid, knots = knots, intercept = TRUE)
y_hat_cv_grid <- predict (cv_fit, newx = x_cv_grid, s = "lambda.1se")
lines (hours_grid, y_hat_cv_grid, lty = 1, col="darkred") #Ridge (glmnet) fitted curve

legend (x="topleft", pch = c(1,1,NA), lty = c(NA, NA, 1), col= c("cornflowerblue", "lightcoral","darkred"), legend = c("Train", "Test", "Ridge (glmnet) Fit"))


```

:::: boxed-text
#### ðŸ’ª Problem 2.3

Repeat Problem 2.2, however, use the optimal $\lambda$ that minimises the mean cross-validated error. Compare the RMSE for the training and test data to those in Problem 2.2.

::: callout-tip
The `help()` function will be very useful here.
:::
::::

```{r}
#=== 1. Prepare data
knots <- seq (0.05, 0.95, length.out = 25)
x_train <- ns (bike_data_train$hour, knots = knots, intercept = TRUE)
x_test <- ns (bike_data_test$hour, knots = knots, intercept = TRUE)

#=== 2. Fit Ridge regression with Cross-validation
set.seed (0)
cv_fit <- cv.glmnet(x = x_train, y = y_train, alpha = 0, nfolds = 10)

#=== 3. Choosing the optimal Lambda using min rule
min_lambda <- cv_fit$lambda.min
cat ("Best lambda (min):", min_lambda, "\n")

#=== 4. Predict fitted values with lambda.min
yhat_train_cv_min <- predict (cv_fit, newx = x_train, s = "lambda.min")
yhat_test_cv_min <- predict (cv_fit, newx = x_test, s = "lambda.min")

#=== 5. Compute RMSE
RMSE_train_cv_min <- sqrt (mean((y_train - yhat_train_cv_min)^2))
RMSE_test_cv_min <- sqrt (mean((y_test - yhat_test_cv_min)^2))
cat("RMSE (Train):", RMSE_train_cv_min, "\n")
cat("RMSE (Test):", RMSE_test_cv_min, "\n")

#=== 6. Plot the fit 
plot (log_cnt ~ hour, data=bike_data_train, col = "cornflowerblue", ylim = c(0,8), main = paste ("Ridge (glmnet) with Î» = ", round(min_lambda,4)))
lines (bike_data_test$hour, bike_data_test$log_cnt, type = "p", col ="lightcoral")

hours_grid <- seq(0,1, length.out = 1000)
x_cv_grid <- ns (hours_grid, knots = knots, intercept = TRUE)
y_hat_cv_grid <- predict (cv_fit, newx = x_cv_grid, s = "lambda.min")
lines (hours_grid, y_hat_cv_grid, lty = 1, col="darkred") #Ridge (glmnet) fitted curve

legend (x="topleft", pch = c(1,1,NA), lty = c(NA, NA, 1), col= c("cornflowerblue", "lightcoral","darkred"), legend = c("Train", "Test", "Ridge (glmnet) Fit"))

#=== 7.Compare results

RMSE_compare <- data.frame (Model = c("Lambda.1se rule", "Lambda.min rule"), RMSE_Train = c(RMSE_train_cv, RMSE_train_cv_min), RMSE_Test = c(RMSE_test_cv, RMSE_test_cv_min))
print (RMSE_compare)

#Conclusion: While both lambda choices yield similar test performance, the Lambda.1se rule leads to a simpler model with slightly better generalization. In contrast, Lambda.min fits the training data better but may overfit slightly, as seen from the marginally higher test RMSE.
```

:::: boxed-text
#### ðŸ’ª Problem 2.4

Repeat Problem 2.2 using L1 regularisation. Compare the RMSE for the training and test data to those in Problem 2.2.

::: callout-tip
The `help()` function will be very useful here to set the argument `alpha` (to the `cv.glmnet()` function).
:::
::::

```{r}

#=== 1. Prepare data
knots <- seq (0.05, 0.95, length.out = 25)
x_train <- ns (bike_data_train$hour, knots = knots, intercept = TRUE)
x_test <- ns (bike_data_test$hour, knots = knots, intercept = TRUE)

#=== 2. Fit Ridge regression with cross-validation (L1 regularisation)
set.seed (0)
cv_model <- cv.glmnet(x = x_train, y = y_train, alpha = 1, nfolds = 10) #set alpha = 1 (LASSO)

#=== 3. Choosing the optimal Lambda using 1-SE rule
opt_lambda_lasso <- cv_model$lambda.1se
cat ("Optimal lambda (1 - SE rule):", opt_lambda_lasso, "\n")

#=== 4. Predict fitted values with lambda.1se
yhat_train_lasso <- predict (cv_model, newx = x_train, s = "lambda.1se")
yhat_test_lasso <- predict (cv_model, newx = x_test, s = "lambda.1se")

#=== 5. Compute RMSE
RMSE_train_lasso <- sqrt (mean((y_train - yhat_train_lasso)^2))
RMSE_test_lasso <- sqrt (mean((y_test - yhat_test_lasso)^2))
cat("RMSE (Train):", RMSE_train_lasso, "\n")
cat("RMSE (Test):", RMSE_test_lasso, "\n")

#=== 6. Plot the fit 
plot (log_cnt ~ hour, data=bike_data_train, col = "cornflowerblue", ylim = c(0,8), main = paste ("Ridge (glmnet) with Î» = ", round(opt_lambda_lasso,3)))
lines (bike_data_test$hour, bike_data_test$log_cnt, type = "p", col ="lightcoral")

hours_grid <- seq(0,1, length.out = 1000)
x_lasso_grid <- ns (hours_grid, knots = knots, intercept = TRUE)
y_hat_lasso_grid <- predict (cv_model, newx = x_lasso_grid, s = "lambda.1se")
lines (hours_grid, y_hat_lasso_grid, lty = 1, col="darkred") #Ridge (glmnet) fitted curve

legend (x="topleft", pch = c(1,1,NA), lty = c(NA, NA, 1), col= c("cornflowerblue", "lightcoral","darkred"), legend = c("Train", "Test", "Ridge (glmnet) Fit"))

#=== 7.Compare results

RMSE_comp <- data.frame (Model = c("L2 regularisation", "L1 regularisation"), RMSE_Train = c(RMSE_train_cv, RMSE_train_lasso), RMSE_Test = c(RMSE_test_cv, RMSE_test_lasso))
print (RMSE_comp)

```

## 3. Regularised regression for bike rental data with more features and data (3 marks)

We now consider the full dataset and use many more features. We use the first one and a half years (Jan 1, 2011 - May 31, 2012) of data for training and the last half year (June 1, 2012- Dec 31, 2012) for testing.

We will use the following categorical features:

-   `weathersit`: Clear (1), cloudy (2), light rain (3), heavy rain (4).

-   `weekday`: Sun (0), Mon (1), Tue (2), Wed (3), Thu (4), Fri (5), Sat (6).

-   `season`: Winter (1), spring (2), summer (3), fall (4).

We will use the common approach in statistics that creates $K-1$ dummy variables for a categorical variable with $K$ levels. The first level is the reference category. The following code constructs these so called one-hot encodings and adds them to the dataset `bike_data`.

```{r}
# One hot for weathersit
one_hot_encode_weathersit <- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)
one_hot_encode_weathersit  <- one_hot_encode_weathersit[, -1] # Remove reference category
colnames(one_hot_encode_weathersit) <- c('cloudy', 'light rain', 'heavy rain')
bike_data <- cbind(bike_data, one_hot_encode_weathersit)

# One hot for weekday
one_hot_encode_weekday <- model.matrix(~ as.factor(weekday) - 1,data = bike_data)
one_hot_encode_weekday  <- one_hot_encode_weekday[, -1] # Remove reference category
colnames(one_hot_encode_weekday) <- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
bike_data <- cbind(bike_data, one_hot_encode_weekday)

# One hot for season
one_hot_encode_season <- model.matrix(~ as.factor(season) - 1,data = bike_data)
one_hot_encode_season  <- one_hot_encode_season[, -1] # Remove reference category
colnames(one_hot_encode_season) <- c('Spring', 'Summer', 'Fall')
bike_data <- cbind(bike_data, one_hot_encode_season)

```

:::: boxed-text
#### ðŸ’ª Problem 3.1

Fit a spline regression to the training data with an L1 regularisation. Find the optimal $\lambda$ by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal $\lambda$) for the training and test data. For the spline, use the `splines` package in R to create natural cubic splines basis functions of the variable `hour` with 10 degrees of freedom, i.e. use the `ns()` function with df=10 as input argument. Moreover, set `intercept=FALSE` and add it manually to your design matrix instead.

The following variables should be included in the regression:

-   Response variable: `log_cnt`.

-   Features: `hour` (via the cubic spline described above), `yr`, `holiday`, `workingday`, `temp`, `atemp`, `hum`, `windspeed`, and all the dummy variables created above.

::: callout-tip
First create the design matrix of interest (you have to exclude the original variables you did the one-hot coding for). When creating the spline basis for the test data, you will need to use the same knots as when constructing the basis functions for the training dataset. If the spline is stored in a variable `spline_basis`, you can extract the knots by using `knots<-attr(spline_basis, "knots")` and then use the `knots` argument when constructing the design matrix for the test data, e.g. `ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)`.
:::
::::

```{r}

#=== 1. Prepare data
bike_alldata_train <- bike_data[bike_data$dteday >= as.Date("2011-01-01") & bike_data$dteday <= as.Date("2012-05-31"), ]
bike_alldata_test <- bike_data[bike_data$dteday >= as.Date("2012-06-01") & bike_data$dteday <= as.Date("2012-12-31"), ]

y_alltrain <- bike_alldata_train$log_cnt
y_alltest <- bike_alldata_test$log_cnt 

#=== 2. Create spline basis for variable hour
spline_train <- ns(bike_alldata_train$hour, df=10, intercept = FALSE)

#=== 3. Extract knots and apply to test set
knots <- attr(spline_train, "knots")
spline_test <- ns (bike_alldata_test$hour, df = 10,knots = knots, intercept = FALSE)

#=== 4. Design matrix for training and test data

x_alltrain <- as.matrix( 
  cbind(1, spline_train, 
  bike_alldata_train[, c('yr','holiday', 'workingday', 'temp', 'atemp', 'hum'
                         ,'windspeed') ], 
  bike_alldata_train [, c('cloudy', 'light rain', 'heavy rain', #weather dummies
                          'Mon', 'Tue', 'Wed', 'Thu', 'Fri','Sat', #weekday
                          'Spring', 'Summer', 'Fall')])) #season dummies

x_alltest <- as.matrix( 
  cbind(1, spline_test, 
  bike_alldata_test[, c('yr','holiday', 'workingday', 'temp', 'atemp', 'hum'
                          ,'windspeed') ], 
  bike_alldata_test [, c('cloudy', 'light rain', 'heavy rain', 
                          'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 
                          'Spring', 'Summer', 'Fall')]))

colnames (x_alltrain) [2:11] <- paste0("Polynomial_", 1:10)
#=== 5. Fit Lasso with cross-validation
set.seed(0)
cv_lasso <- cv.glmnet(x_alltrain, y_alltrain, alpha = 1, nfolds = 10, intercept = FALSE) 

#=== 6. Choosing the best lambda
best_lambda_lasso <- cv_lasso$lambda.1se
cat ("Optimal lambda (L1, 1-SE):", best_lambda_lasso, "\n")

#=== 7. Compute the RMSE
y_alltrain_hat <- predict(cv_lasso, newx = x_alltrain, s = "lambda.1se")
y_alltest_hat <- predict (cv_lasso, newx = x_alltest, s = "lambda.1se")

RMSE_train_lasso <- sqrt(mean((y_alltrain - y_alltrain_hat)^2))
RMSE_test_lasso <- sqrt(mean((y_alltest - y_alltest_hat)^2))
cat("RMSE (Train):", RMSE_train_lasso, "\n")
cat ("RMSE (Test):", RMSE_test_lasso, "\n")
```

:::: boxed-text
#### ðŸ’ª Problem 3.2

Which three features in Problem 3.1 seem to be the most important?

::: callout-tip
To explore which variables are most important, you can re-estimate the model in Problem 3.1 without cross-validation (using `glmnet()`). Note that you can apply the `plot()` function to an object returned by `glmnet()` using the arguments `xvar="lambda"` or `xvar="norm"`, and `label="TRUE"`, which allows you to visualise the lasso path.
:::
::::

```{r}
#=== 1. Re-estimate the model without cross-validation
set.seed (0)
lasso_model <- glmnet(x = x_alltrain, y = y_alltrain, alpha = 1, intercept = FALSE)
#=== 2. Plot LASSO coefficient paths
#plot (lasso_model, xvar = "norm", label = "TRUE")
#title ("LASSO path - Most Important Features", line = 3 )

suppressMessages(library(plotmo))

plot_glmnet (lasso_model, xvar = "norm", type = "1", main = "LASSO path")

#=== 3. Top coefficients

coefs_model <- coef(lasso_model, s = best_lambda_lasso)
coefs <- as.vector(coefs_model)[-1]
names(coefs) <- rownames(coefs_model)[-1]

  #--- Sort coefficents by absolute size
  Sort <- sort(abs(coefs), decreasing = TRUE)

  #--- Print out the coefficients of most important features
  top3 <- head(Sort, 3)
  top_features  <- data.frame(
    Features = names(top3),
    Coefs_abs = top3,
    row.names = NULL
  )
  print(top_features)

```

:::: boxed-text
#### ðŸ’ª Problem 3.3

Carry out a residual analysis in Problem 3.1 for the training data. What can you say about the assumption of independent residuals? Repeat the same for the test data.

::: callout-tip
Plot the autocorrelation function of the residuals to validate the independence assumption.
:::
::::

```{r}
#=== 1. Compute the residuals 
residuals_train <- y_alltrain - y_alltrain_hat
residuals_test <- y_alltest - y_alltest_hat

#=== 2. Plot of autocorrelation function 
acf(residuals_train, main = "ACF of residuals - training data")
acf(residuals_test, main = "ACF of residuals - test data")

#=== 3. Plot the residual plot

plot(residuals_train, type = "l", col = "darkblue",
     main = "Residuals (train) over Observation Index",
     xlab = "Index (Time order)", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

plot(residuals_test, type = "l", col = "darkblue",
     main = "Residuals (test) over Observation Index",
     xlab = "Index (Time order)", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

## 4. Regularised time series regression and regression trees for bike rental data (4 marks)

In this section we will consider the time series nature of the data.

::: boxed-text
#### ðŸ’ª Problem 4.1

Plot a time series plot of the response in the original scale (i.e. counts and not log-counts) for the last week of the test data (last $24\times 7$ observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 3.1. Comment on the fit.
:::

```{r}
#=== 1. Extract the last week of the test data
lastweek_idx <- 24*7
data_last <- tail (bike_alldata_test, lastweek_idx)
true_count <- data_last$cnt

#=== 2. Compute the last 168 fitted values from Problem 3.1
fitted_cnts <- exp(tail(y_alltest_hat, lastweek_idx)) #back-transformation predictions to original scale

#last_week_data <- (nrow(bike_alldata_test) - 167):nrow(bike_alldata_test)
#y_test_lastweek <- bike_alldata_test$cnt[last_week_data] #true counts
#hour_lastweek <- bike_alldata_test$hr[last_week_data] #extract the corresponding hour 

#=== 2. Compute the fitted values from Problem 3.1
#y_alltest_hat <- predict (cv_lasso, newx = x_alltest, s = "lambda.1se")
#y_hat_cnt <- exp (y_alltest_hat) #back-transformation predictions to original scale
#y_hat_lastweek <- y_hat_cnt [last_week_data] #Extract last 168 fitted values 

#=== 3. Plot the time series
plot_grid <- 1:lastweek_idx
plot (plot_grid, true_count, type = "l", lwd = 2, col = "darkblue",lty = 1, xlab = "Hour", ylab = "Bike rentals (count)", main = "Actual vs Fitted values (last week)")
lines (plot_grid, fitted_cnts, col ="da rkred", lty = 2, lwd = 2)
legend (x = "topleft", col = c("darkblue", "darkred"), lty = c(1,2), legend = c("Actual", "Predicted"))

#=== 4. Comment on the fit:The predicted values track the overall trend of the actual values well, especially for the peaks and troughs across the 24-hour cycle. This indicates that the model captures the general pattern of bike rentals, such as the morning and evening peaks and the dips during late night and early morning. 


```

z

:::: boxed-text
#### ðŸ’ª Problem 4.2

Add time series effects to your model by including some lagged values of the response as features. Use the first four hourly lags of `log_cnt` plus the 24th hourly lag as features, in addition to all other features in Problem 3.1. Fit the model using an L1 regularisation and find the optimal $\lambda$ by applying the one-standard deviation rule when cross-validating the training data using 10 folds. Compute the RMSE (using the optimal $\lambda$) for the training and test data and compare to Problem 3.1. Are the residuals from this new model more adequate?

::: callout-tip
The function `mutate()` from the `dplyr` package is useful for adding columns to a data frame. Moreover, the function `lags(x, k)` lags the time series `x` by `k` steps.

Notice that some observations are lost when lagging, i.e. `NA` values are introduced. You can remove these observations from the dataset.
:::
::::

```{r}
suppressMessages(library(dplyr))

#=== 1. Prepare data
  #--- Combine train + test log_cnt
  #combined_data <- bind_rows (bike_alldata_train, bike_alldata_test)
  #--- Create lagged features and add to the full data set
  combined_data <- bike_data %>% arrange (as.Date(dteday), hr) %>%
    mutate (log_cnt_lag1 = lag(log_cnt, 1), #lag 1 hour
          log_cnt_lag2 = lag (log_cnt, 2), #lag 2 hour
          log_cnt_lag3 = lag (log_cnt, 3), #lag 3 hour
          log_cnt_lag4 = lag (log_cnt, 4), #lag 4 hour
          log_cnt_lag24 = lag (log_cnt, 24)) #lag 1 day
  
  #--- Split back to train and test data
  bike_data_train_lag <- combined_data %>% filter(dteday >= as.Date("2011-01-01") & dteday <= as.Date("2012-05-31"))
  bike_data_test_lag  <- combined_data %>% filter(dteday >= as.Date("2012-06-01") & dteday <= as.Date("2012-12-31"))
  
  #--- Remove rows with NA values (resulting from lagging)
  bike_data_train_lag <- na.omit(bike_data_train_lag)
  bike_data_test_lag <- na.omit(bike_data_test_lag)

  #--- Define response
  y_train_lag <- bike_data_train_lag$log_cnt
  y_test_lag <- bike_data_test_lag$log_cnt

#=== 3. Spline basis train for variable hour (train)
spline_train_lag <- ns(bike_data_train_lag$hour, df=10, intercept = FALSE)

  #--- Extract knots to apply on test set
  knots <- attr(spline_train_lag, "knots")
  spline_test_lag <- ns(bike_data_test_lag$hour, df = 10, knots = knots, intercept = FALSE)

#=== 5. Redesign matrix that include lagged features
x_train_lag <- as.matrix( 
  cbind(1, spline_train_lag, 
        bike_data_train_lag[, c('log_cnt_lag1', 'log_cnt_lag2', 'log_cnt_lag3',
                            'log_cnt_lag4', 'log_cnt_lag24')],  # Lagged features
        bike_data_train_lag[, c('yr','holiday', 'workingday', 'temp', 'atemp', 'hum'
        ,'windspeed') ], 
        bike_data_train_lag [, c('cloudy', 'light rain', 'heavy rain', #weather dummies
                          'Mon', 'Tue', 'Wed', 'Thu', 'Fri','Sat', #weekday
                          'Spring', 'Summer', 'Fall')])) #season dummies

x_test_lag <- as.matrix( 
  cbind(1, spline_test_lag, 
        bike_data_test_lag[, c('log_cnt_lag1', 'log_cnt_lag2', 'log_cnt_lag3',
                            'log_cnt_lag4', 'log_cnt_lag24')],  # Lagged features
        bike_data_test_lag[, c('yr','holiday', 'workingday', 'temp', 'atemp', 'hum'
        ,'windspeed') ], 
        bike_data_test_lag [, c('cloudy', 'light rain', 'heavy rain', #weather dummies
                          'Mon', 'Tue', 'Wed', 'Thu', 'Fri','Sat', #weekday
                          'Spring', 'Summer', 'Fall')])) #season dummies

#=== 5. Fit the LASSO model with cross-validation
set.seed(0)
cv_fit_lag <- cv.glmnet (x_train_lag, y_train_lag, alpha = 1, nfolds = 10, intercept = FALSE)
lambda_lag <- cv_fit_lag$lambda.1se
cat ("Lambda",lambda_lag, "\n")

y_hat_train_lag <- predict (cv_fit_lag, newx = x_train_lag, s = lambda_lag)
y_hat_test_lag <- predict (cv_fit_lag, newx = x_test_lag, s = lambda_lag) 

#=== 6. Compute RMSE 

RMSE_train_lag <- sqrt(mean((y_train_lag - y_hat_train_lag)^2))
RMSE_test_lag <- sqrt(mean((y_test_lag - y_hat_test_lag)^2))
cat("RMSE (Train):", RMSE_train_lag, "\n")
cat ("RMSE (Test):", RMSE_test_lag, "\n")

#=== 7. Compare to Problem 3.1
RMSE_comps <- data.frame (Model = c("L1 regularisation (Lagged model)", "L1 regularisation"), RMSE_Train = c(RMSE_train_lag, RMSE_train_lasso), RMSE_Test = c(RMSE_test_lag, RMSE_test_lasso))
print (RMSE_comps)

#=== 8. Compute the residuals
residuals_train_lag <- y_train_lag - y_hat_train_lag
residuals_test_lag <- y_test_lag - y_hat_test_lag
  #---Plot of autocorrelation function 
  acf(residuals_train_lag, main = "ACF of residuals - training data")
  acf(residuals_test_lag, main = "ACF of residuals - test data")
  
  #---Plot the residuals plot
  plot(residuals_train_lag, type = "l", col = "darkblue",
    main = "Residuals (train) over Observation Index",
    xlab = "Index (Time order)", ylab = "Residuals")
    abline(h = 0, col = "red", lty = 2)
  
  plot(residuals_test_lag, type = "l", col = "darkblue",
    main = "Residuals (test) over Observation Index",
    xlab = "Index (Time order)", ylab = "Residuals")
    abline(h = 0, col = "red", lty = 2)
```

::: boxed-text
#### ðŸ’ª Problem 4.3

Add the predictions from Problem 4.2 to the figure you created in Problem 4.1. Did the predictions improve by adding lags of the response variable?
:::

```{r}
#=== 1. Fitted values from problem 4.1
fitted_cnts <- exp(tail(y_alltest_hat, lastweek_idx))

#last_week_data_lag <- (nrow(bike_alldata_test_lag) - 24*7 +1 ):nrow(bike_alldata_test_lag)
#y_test_lastweek_lag <- bike_alldata_test_lag$cnt[last_week_data_lag] #true counts

#=== 2. Compute the fitted values from Problem 4.2
fitted_cnts_lag <- exp (tail(y_hat_test_lag, lastweek_idx))
#y_hat_lastweek_lag <- y_hat_cnt_lag [last_week_data_lag] #Extract last 168 fitted values
#=== 2. Add the prediction to the plot
plot_grid <- 1:lastweek_idx
plot (plot_grid, true_count, type = "l", lwd = 1, col = "darkblue",lty = 1, xlab = "Hour", ylab = "Bike rentals (count)", main = "Actual vs Fitted values (last week)")
lines (plot_grid, fitted_cnts, col ="darkred", lty = 2, lwd = 1)
lines (plot_grid, fitted_cnts_lag, col ="darkgreen", lty = 3, lwd = 2)

legend (x = "topleft", col = c("darkblue", "darkred", "darkgreen"), lty = c(1,2,3), legend = c("Actual", "Predicted", "Predicted with lags"))

```

::: boxed-text
#### ðŸ’ª Problem 4.4

Another alternative to the semi-parametric approach above is to fit a regression tree.

Using the training dataset from Problem 4.2 (that also includes lagged values of the response as features), fit a regression tree using the `tree` package. If you want, experiment with the settings to see how changing them affects the results.
:::

```{r}
suppressMessages(library(tree))

#=== 1. Prepare data 
  # --- Name the splines column
splines_names <- paste0("spline", 1:10)
colnames (spline_train_lag) <- splines_names
colnames (spline_test_lag) <- splines_names

  # --- Add spline columns to data
bike_data_train_lag <- cbind (bike_data_train_lag, spline_train_lag)
bike_data_test_lag <- cbind (bike_data_test_lag, spline_test_lag)

  #--- Define features for tree

features <- c ('log_cnt_lag1', 'log_cnt_lag2', 'log_cnt_lag3', 
              'log_cnt_lag4',  'log_cnt_lag24',
               'yr','holiday', 'workingday', 'temp', 'atemp', 'hum','windspeed',
               'cloudy', 'light rain', 'heavy rain',
               'Mon', 'Tue', 'Wed', 'Thu', 'Fri','Sat', 
               'Spring', 'Summer', 'Fall', splines_names)

  #--- Ensure valid names
colnames (bike_data_train_lag) <- make.names(colnames(bike_data_train_lag))
colnames (bike_data_test_lag) <- make.names(colnames(bike_data_test_lag))
features <- make.names (features)

#=== 2. Prepare tree data
treedata_train <- (bike_data_train_lag[, c('log_cnt', features)])
treedata_train <- na.omit(treedata_train)
treedata_test <- bike_data_test_lag[, features]
  
y_train_tree <- treedata_train$log_cnt
y_test_tree <- bike_data_test_lag$log_cnt

#=== 3. Set tree controls/experiment parameters
#minisize = minimum observations in a node to allow split
#mindev = minimum deviace reduction to allow split

tree_ctrl <- tree.control(nobs = nrow(treedata_train), minsize = 5, mindev = 0.001)

#=== 4.Fit the regression tree
set.seed(0)
regression_tree <- tree (log_cnt ~ ., data = treedata_train)

#=== 5. Inspect the tree
print(summary(regression_tree)) # summary

#=== 6. Prediction from the tree and compute RMSE
y_hat_train_tree <- predict(regression_tree, newdata = treedata_train)
y_hat_test_tree <- predict(regression_tree, newdata = treedata_test)

RMSE_train_tree <- sqrt(mean((y_train_tree - y_hat_train_tree)^2))
RMSE_test_tree <- sqrt(mean((y_test_tree - y_hat_test_tree)^2))
cat("RMSE (Train):", RMSE_train_tree, "\n")
cat ("RMSE (Test):", RMSE_test_tree, "\n")
```

::: boxed-text
#### ðŸ’ª Problem 4.5

Plot the tree structure in Problem 4.4.
:::

```{r}
plot (regression_tree) 
text (regression_tree, pretty = 3, cex = 0.7)
title ("Full tree structure")

#=== Optional: prune tree by specifying max depth
# Prune based on CV error to avoid overfitting
cv_tree <- cv.tree(regression_tree, K = 10)

# Keep only unique tree sizes and corresponding deviance
unique_sizes <- !duplicated(cv_tree$size)
cv_size <- cv_tree$size[unique_sizes]
cv_dev <- cv_tree$dev[unique_sizes]

# Plot 
plot(cv_size, cv_dev, type='b', xlab="Tree size (terminal nodes)", ylab="CV Deviance")

# Find optimal tree size
opt_size <- cv_size[which.min(cv_dev)]
print(opt_size)

# Prune the tree to optimal size
pruned_tree <- prune.tree(regression_tree, best = opt_size)
plot(pruned_tree)
text(pruned_tree, pretty=0, cex=0.7)
```

::: boxed-text
#### ðŸ’ª Problem 4.6

Add the predictions from Problem 4.4 to the figure you created in Problem 4.3. Comment on the fit of the regression tree compared to that of the semi-parametric spline approach.
:::

```{r}
#=== 1. Compute predicted values from problem 4.5
y_hat_test_tree <- predict(regression_tree, newdata = treedata_test)
fitted_cnts_tree <- exp (tail (y_hat_test_tree, lastweek_idx)) #Values for the data of last week

#last_week_data_lag <- (nrow(bike_alldata_test_lag) - 24*7 +1 ):nrow(bike_alldata_test_lag)
#y_test_lastweek_lag <- bike_alldata_test_lag$cnt[last_week_data_lag] #true count

#=== 2. Predict values for the last week
#y_hat_tree_lastweek <- predict (regression_tree, newdata = treedata_test_lastweek)
#y_hat_cnt_tree <- exp (y_hat_tree_lastweek) #Transform back to original scale

#=== 2. Add prediction to the plot
plot_grid <- 1:168
plot (plot_grid, true_count, type = "l", lwd = 1, col = "darkblue",lty = 1, xlab = "Hour", ylab = "Bike rentals (count)", main = "Actual vs Fitted values (last week)")
lines (plot_grid, fitted_cnts, col ="darkred", lty = 2, lwd = 1)
lines (plot_grid, fitted_cnts_lag, col ="forestgreen", lty = 3, lwd = 2)
lines (plot_grid, fitted_cnts_tree, col = "black", lty = 3, lwd = 2)

legend (x = "topleft", col = c("darkblue", "darkred", "forestgreen","black"), lty = c(1,2,3,3), legend = c("Actual", "Predicted", "Predicted with lags", "Regression tree"))



```

## 5. Logistic regression and decision trees for classifying spam emails (3 marks)

The dataset `spam_ham_emails.RData` consists of $n=4601$ spam ($y=1$) and ham (no spam, $y=0$) emails with corresponding 15 features[^2].

[^2]: The original data come from this [source](https://archive.ics.uci.edu/ml/datasets/spambase). The dataset used here includes a subset of the 57 original features.

Most of the features are continuous real variables in the interval $[0, 100]$, with values corresponding to the percentage of occurrence of a specific word or character in the email. There are also a few features that capture the tendency to use many capital letters. The features are on different scales so we will standardise them to have zero mean and unit variance. The following code reads in the data (note that you have to change to the path were you stored the file!), standardises the features, and codes the response variable as a factor.

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
load(file = 'C:/Users/TSC/Desktop/UTS/Sem 1/Machine Learning/Cmp Lab 1/spam_ham_emails.RData')
Spam_ham_emails[, -1] <- scale(Spam_ham_emails[, -1])
Spam_ham_emails['spam'] <- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1->TRUE, 0->FALSE
levels(Spam_ham_emails$spam) <- c("not spam", "spam")
head(Spam_ham_emails)
str(Spam_ham_emails)
```

The response is the first column `spam` and we will use all the other features to predict if an email is a spam or ham. Let us first inspect the fraction of spam emails in the dataset.

```{r}
cat("Percentage of spam:", 100*mean(Spam_ham_emails$spam == "spam"))
```

We now split the data using the `createDataPartition()` function in the `caret` package. The default method in `caret` is stratified sampling, which keeps the same fraction of spam and ham in both the training and test datasets. We use 75% of the data for training and 25% for testing.

```{r}
set.seed(1234)
suppressMessages(library(caret))
train_obs <- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)
train <- Spam_ham_emails[train_obs, ]
test <- Spam_ham_emails[-train_obs, ]
# Confirm both training and test are balanced with respect to spam emails
cat("Percentage of training data consisting of spam emails:", 
              100*mean(train$spam == "spam"))
cat("Percentage of test data consisting of spam emails:", 
              100*mean(test$spam == "spam"))
```

The following code fits a logistic regression on the training data using the `glm()` function, predicts the test data following the rule if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and computes the confusion matrix using the `caret` package.

```{r}
glm_fit <- glm(spam ~ ., family = binomial, data = train)
y_prob_hat_test <- predict(glm_fit, newdata = test, type = "response")
threshold <- 0.5 # Predict spam if probability > threshold
y_hat_test <- as.factor(y_prob_hat_test > threshold)
levels(y_hat_test) <- c("not spam", "spam")
confusionMatrix(data = y_hat_test, test$spam, positive = "spam")
```

::: boxed-text
#### ðŸ’ª Problem 5.1

Reconstruct the confusion matrix for the test data without using the `confusionMatrix()` function.
:::

```{r}
y_true <- as.character(test$spam)
#=== 1. Predicted labels using threshold
y_pred <- ifelse (y_prob_hat_test > threshold, "spam", "not spam")

#=== 2. Calculate the TP, TN, FP, FN
TP <- sum (y_pred == "spam" & y_true == "spam")
TN <- sum (y_pred == "not spam" & y_true == "not spam" )
FP <- sum (y_pred == "spam" & y_true == "not spam")
FN <- sum (y_pred == "not spam" & y_true == "spam")

#=== 3. Build confusion matrix
conf_matrix <- matrix(c (TN, FN, FP, TP), nrow = 2, byrow = TRUE, dimnames = list ("Predicted" = c ("not spam", "spam"), "Actual" = c ("not spam", "spam")))

print("Confusion Matrix (Actual vs Predicted):")
print(conf_matrix)
```

::: boxed-text
#### ðŸ’ª Problem 5.2

Compute the accuracy, precision, sensitivity (recall), and specificity without using the `confusionMatrix()` function. Explain these four concepts in the context of the spam filter.
:::

```{r}
#=== 1. Compute the accuracy, precision, sensitivity (recall) and specificity

accuracy    <- (TP + TN) / (TP + TN + FP + FN)
precision   <- TP / (TP + FP)
recall      <- TP / (TP + FN)    # sensitivity
specificity <- TN / (TN + FP)

metrics_log <- data.frame(
  Metrics = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity"),
  Logistic  = c(accuracy, precision, recall, specificity)
)
metrics_log$Logistic <- round(metrics_log$Logistic, 4)
print(metrics_log, row.names = FALSE)

#=== 2. Explain the four concepts




```

::: boxed-text
#### ðŸ’ª Problem 5.3

Compute the receiver operating characteristic (ROC) curve using the `pROC`. Explain in detail what the ROC curve shows.
:::

```{r}
# Load required library
suppressMessages(library(PRROC))
suppressMessages(library(pROC))
#=== 1. Compute the ROC 
ref <- factor(test$spam, levels = c("not spam", "spam"))
roc_cal <- roc(response = ref,
               predictor = y_prob_hat_test,
               levels = c("not spam", "spam"), 
               direction = "<") #higher scores correspond to the positive class

#=== 2. Plot the ROC curve
plot(roc_cal,
     main = "ROC Curve - Spam Filter",
     col = "blue",
     lwd = 2)

#=== 3. Compute the AUC
auc (roc_cal)

#=== Optional
# 1) Convert actual labels to numeric: spam = 1, not spam = 0
y_true <- ifelse(test$spam == "spam", 1, 0)

# 2) Compute precision-recall curve points
pr <- pr.curve(scores.class0 = y_prob_hat_test[y_true == 1],
               scores.class1 = y_prob_hat_test[y_true == 0],
               curve = TRUE)

# 3) Plot Precisionâ€“Recall curve
plot(pr, 
     main = sprintf("Precisionâ€“Recall Curve (AUC = %.3f)", pr$auc.integral),
     xlab = "Recall (Sensitivity)",
     ylab = "Precision")

#=== 3. Explain in details
```

::: boxed-text
#### ðŸ’ª Problem 5.4

Using the training dataset, fit a decision tree (classification tree) using the `tree` package with the default settings. How does this classifier perform on the test dataset compared to the logistic classifier?
:::

```{r}
suppressMessages(library(tree))
suppressMessages(library(caret))

#=== 1. Fit the tree
set.seed (0)
tree_classification <- tree(spam ~., data = train)

y_hat_tree <- predict (tree_classification, newdata = test, type = "class")

ref <- factor (test$spam, levels = c ("not spam", "spam"))

#=== 2. Compute the confusion matrix

conf_matrix_tree <- confusionMatrix(data = y_hat_tree, reference = ref, positive = "spam")
  #--- Extract key metrics
  ext_metrics <- function (conf_matrix_tree) {
    data.frame ( c(
                Accuracy = as.numeric (conf_matrix_tree$overall["Accuracy"]),
                Precision = as.numeric(conf_matrix_tree$byClass["Pos Pred Value"]),                 Recall= as.numeric (conf_matrix_tree$byClass["Sensitivity"]),                      Specificity = as.numeric (conf_matrix_tree$byClass["Specificity"])))}
 
  metrics_tree <- ext_metrics(conf_matrix_tree)
  metrics_tree <- data.frame (
                  Metrics = c("Accuracy", "Precision", "Recall (Sensitivity)", "Specificity"),
                  DecisionTree = unname (metrics_tree),
                  row.names = NULL )
  
  
#=== 3. Compare with metrics of logistic classifier
comparison <- merge (metrics_tree, metrics_log, by = "Metrics", all = TRUE )
comparison$Logistic <- round (comparison$Logistic, 4)
comparison$DecisionTree <-round (comparison$DecisionTree, 4)
  
print (comparison)
```

::: boxed-text
#### ðŸ’ª Problem 5.5

Plot the tree structure in Problem 5.4.
:::

```{r}
plot (tree_classification)
text (tree_classification, pretty = 0, cex = 0.7)

print (summary(tree_classification))
```
